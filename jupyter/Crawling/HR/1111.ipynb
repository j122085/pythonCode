{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in d:\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter # import submodule at top level namesapce\n",
    "import datetime as dt # import with an alias name\n",
    "from collections import Counter as Ctr # import submodule at top level namespace with an alias\n",
    "\n",
    "import jieba\n",
    "from jieba.analyse import extract_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba.set_dictionary('./dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./cai_speech.txt') as f:\n",
    "    speech = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_count = Counter()\n",
    "for word in jieba.cut(speech):\n",
    "    print(word)\n",
    "    if word in wd_count:\n",
    "        wd_count[word] += 1\n",
    "    else:\n",
    "        wd_count[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extract_tags(speech,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages\n",
      "Requirement already satisfied: urllib3<1.22,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.6,>=2.5 in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages (from requests)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/ubuntu/.pyenv/versions/3.6.1/lib/python3.6/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "need = Counter()\n",
    "for i in range(1,150):\n",
    "    res=requests.get('https://www.104.com.tw/jobbank/joblist/joblist.cfm?keyword=%E8%BB%9F%E9%AB%94&jobsource=n104bank1&ro=0&order=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\".job_name > a\"):\n",
    "        hr=job['href']\n",
    "        jk=hr.split('jobno=')[1].split('&jobsource')[0]\n",
    "    #         print(jk)\n",
    "        res2=requests.get('https://www.104.com.tw/job/?jobno='+jk+'&jobsource=104_hotorder')\n",
    "        soup=BeautifulSoup(res2.text,'lxml')\n",
    "        for goodat in soup.select('.tool'):\n",
    "            for w in goodat.select('a'):\n",
    "                if w.text in need:\n",
    "                    need[w.text]+=1\n",
    "                else:\n",
    "                    need[w.text]=1\n",
    "print(need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Java', 10), ('Linux', 9), ('軟體程式設計', 9), ('C#', 8), ('C', 7), ('C++.Net', 6), ('C++', 6), ('Tomcat', 5), ('網路程式設計', 5), ('Visual C++', 5), ('軟體工程系統開發', 5), ('Android', 5), ('Firewall', 5), ('FTP', 5), ('HTTP', 5), ('TCP/IP', 5), ('通訊技術發展研究', 5), ('資料通訊與網路應用', 5), ('功能測試(function test)', 5), ('安全性測試(Security test)', 5)]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "need = Counter()\n",
    "for i in range(1,2):\n",
    "    res=requests.get('https://www.104.com.tw/jobbank/joblist/joblist.cfm?keyword=%E8%BB%9F%E9%AB%94&jobsource=n104bank1&ro=0&order=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    #print(soup)\n",
    "    for job in soup.select(\".job_name > a\"):\n",
    "        hr=job['href']\n",
    "        a=hr.split('jobno=')[1].split('&jobsource')[0]\n",
    "        \n",
    "        res2=requests.get('https://www.104.com.tw/job/?jobno='+a+'&jobsource=104_hotorder')\n",
    "        soup=BeautifulSoup(res2.text,'lxml')\n",
    "        for goodat in soup.select('.tool > a'):\n",
    "            var=goodat.text\n",
    "            if var in need:\n",
    "                need[var]+=1\n",
    "            else:\n",
    "                need[var]=1\n",
    "print(need.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(goodat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "need = Counter()\n",
    "bad=['軟體程式設計','軟體工程系統開發','網路程式設計','功能測試(function test)','網路程式設計']\n",
    "for i in range(1,150):\n",
    "    res=requests.get('https://www.104.com.tw/jobbank/joblist/joblist.cfm?keyword=%E8%BB%9F%E9%AB%94&jobsource=n104bank1&ro=0&order=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    #print(soup)\n",
    "    for job in soup.select(\".job_name > a\"):\n",
    "        hr=job['href']\n",
    "        a=hr.split('jobno=')[1].split('&jobsource')[0]\n",
    "        \n",
    "        res2=requests.get('https://www.104.com.tw/job/?jobno='+a+'&jobsource=104_hotorder')\n",
    "        soup=BeautifulSoup(res2.text,'lxml')\n",
    "        for goodat in soup.select('.tool > a'):\n",
    "            var=goodat.text\n",
    "            if var in bad:\n",
    "                pass\n",
    "            elif var in need:\n",
    "                need[var]+=1\n",
    "            else:\n",
    "                need[var]=1\n",
    "print(need.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "need = Counter()\n",
    "bad=['軟體程式設計','軟體工程系統開發','網路程式設計','功能測試(function test)','網路程式設計']\n",
    "for i in range(1,2):\n",
    "    res=requests.get('https://www.104.com.tw/jobbank/joblist/joblist.cfm?keyword=%E8%BB%9F%E9%AB%94&jobsource=n104bank1&ro=0&order=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    #print(soup)\n",
    "    for job in soup.select(\".job_name > a\"):\n",
    "        hr=job['href']\n",
    "        #print(hr)\n",
    "        #a=hr.split('jobno=')[1].split('&jobsource')[0]\n",
    "        res2=requests.get('https://www.104.com.tw'+hr)\n",
    "        soup=BeautifulSoup(res2.text,'lxml')\n",
    "        for goodat in soup.select('.tool > a'):\n",
    "            var=goodat.text\n",
    "            if var in bad:\n",
    "                pass\n",
    "            elif var in need:\n",
    "                need[var]+=1\n",
    "            else:\n",
    "                need[var]=1\n",
    "print(need.most_common(10))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "\n",
    "need = Counter()\n",
    "bad=['軟體程式設計','軟體工程系統開發','網路程式設計','功能測試(function test)','網路程式設計']\n",
    "for i in range(1,2):\n",
    "    res=requests.get('https://www.104.com.tw/jobbank/joblist/joblist.cfm?keyword=%E8%BB%9F%E9%AB%94&jobsource=n104bank1&ro=0&order=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    #print(soup)\n",
    "    for job in soup.select(\".job_name > a\"):\n",
    "        hr=job['href']\n",
    "        print(hr)\n",
    "with open(./fhwhf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo 'Hello World! My name is Frank Lu . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 巨量分析 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Python': 3, 'LINUX': 2, 'JAVA': 2, 'ASP.NET': 1, 'PHP': 1, 'MySQL': 1, 'Oracle': 1, 'C/C++': 1, 'Java': 1, 'Script': 1, 'Word': 1, 'Excel': 1, 'PowerPoint': 1})\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "print(need)\n",
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_巨量分析.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1111 巨量分析 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "\n",
    "with open('a1111_巨量分析.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 軟體工程 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%BB%9F%E9%AB%94%E5%B7%A5%E7%A8%8B&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%BB%9F%E9%AB%94%E5%B7%A5%E7%A8%8B&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_軟體工程.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1111 軟體工程 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%BB%9F%E9%AB%94%E5%B7%A5%E7%A8%8B&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%BB%9F%E9%AB%94%E5%B7%A5%E7%A8%8B&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "with open('a1111_軟體工程.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 資料庫 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%BA%AB&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%BA%AB&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_資料庫.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1111 資料庫 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%BA%AB&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%BA%AB&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "with open('a1111_資料庫.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 DataAnalyst csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=DATA%20ANALYST&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=DATA%20ANALYST&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_DataAnalyst.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1111 DataAnalyst json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=DATA%20ANALYST&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=DATA%20ANALYST&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "with open('a1111_DataAnalyst.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 資料工程 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_資料工程.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 資料工程 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=%E8%B3%87%E6%96%99%E5%B7%A5%E7%A8%8B&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "with open('a1111_資料工程.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1111 Big Data json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=BIG%20DATA&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ks=BIG%20DATA&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "\n",
    "with open('a1111_BigData.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1111 Hadoop json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&d0=140202,140210,140209,140402,140301,140302&fs=1&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&d0=140202,140210,140209,140402,140301,140302&fs=1&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "\n",
    "with open('a1111_Hadoop.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 測試區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&ss=s&ks=%E5%B7%A8%E9%87%8F%E8%B3%87%E6%96%99&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "#print(need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5211be6ef5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'b'"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "for i in need:\n",
    "    a.append(i)\n",
    "    b.append(str(need[i]))\n",
    "\n",
    "with open('a1111_巨量分析2.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ASP.NET': 1,\n",
       "         'C/C++': 1,\n",
       "         'Excel': 1,\n",
       "         'JAVA': 2,\n",
       "         'Java': 1,\n",
       "         'LINUX': 2,\n",
       "         'MySQL': 1,\n",
       "         'Oracle': 1,\n",
       "         'PHP': 1,\n",
       "         'PowerPoint': 1,\n",
       "         'Python': 3,\n",
       "         'Script': 1,\n",
       "         'Word': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need\n",
    "with open('a1111_巨量分析2.csv', 'w') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(a)\n",
    "        w.writerow(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import requests\n",
    "import json\n",
    "\n",
    "need = Counter()\n",
    "res3=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&d0=140202,140210,140402&fs=1&ps=100&page=1')\n",
    "soup3=BeautifulSoup(res3.text,'lxml')\n",
    "y = soup3.select('.pagedata')\n",
    "n = int(y[0].text.split('/')[1].split('頁')[0])\n",
    "for i in range(1,n):\n",
    "    res=requests.get('https://www.1111.com.tw/job-bank/job-index.asp?si=1&d0=140202,140210,140402&fs=1&ps=100&page='+str(i))\n",
    "    soup=BeautifulSoup(res.text,'lxml')\n",
    "    for job in soup.select(\"#jobResult\"):\n",
    "        #print(job)\n",
    "        b = job.select('h3 > a')\n",
    "        for a in b:\n",
    "            hrf = a['href']\n",
    "            #print(hrf)\n",
    "            res2 = requests.get(\"https:\"+hrf)\n",
    "            soup2 = BeautifulSoup(res2.text,'lxml')\n",
    "            for row in soup2.select('.dataList > li'):\n",
    "                ab = row.text\n",
    "#                 print(ab)\n",
    "#                 print(\"=\"*50)\n",
    "                if ab[:5] == '電腦專長：':\n",
    "                    var = ab[5:].replace('、',' ').split()\n",
    "                    #print(var)\n",
    "                    for vstr in var:\n",
    "                        if vstr in need:\n",
    "                            need[vstr]+=1\n",
    "                        else:\n",
    "                            need[vstr]=1\n",
    "\n",
    "with open('a1111_Hadoop.json','w') as f:\n",
    "    json.dump(need,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

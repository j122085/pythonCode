{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'台南食的精采應該不需多加介紹,眾所皆知,光一個早餐就包羅萬象老婆說得好:「難得來一趟,早餐要吃道地的.....」沒錯,吃完矮仔成蝦仁飯還不夠,另外一個早餐行程【阿堂鹹粥】規律的上班族,假日閒暇時,帶著相機....我是食客也是旅人,拍下生活也記錄值得推薦的吃與玩。分店開張，歡迎作客~來到西門圓環附近,車龍開始龜速.....遠遠一看....和料想的一樣,果然【阿堂鹹粥】又爆滿了經歷了六千牛肉湯90分鐘排隊的洗禮.....眼前這爆滿人群的狀態,咱家似乎免疫了,排隊.....小意思....無感....所以,阿堂鹹粥不難找,塞車了....表示快到了....冏Orz而且預告:排隊吧!食客賣的品項不算太多,但今天能吃到的更少鹹粥類只有綜合鹹粥,虱目魚肚已經售完送餐、掌廚、點菜....裏裏外外店員10來個,才能應付這龐大的消費群雖然人多,不過翻桌速度也快,約10分鐘後就有座位疑~不是剛吃過矮仔成蝦仁飯嗎!?沒錯,阿堂鹹粥也賣蝦仁飯,可見這碗飯魅力之大阿堂鹹粥的蝦仁飯size比較小,蝦仁也來的少,當然價格也較便宜些攤位上就能看到已經有好幾碗蝦仁飯預先做好,所以飯粒有些硬,溫度有些涼蝦仁的口感還不錯,但香氣就沒有矮仔成來得濃郁,不過還不錯吃若單純以蝦仁飯來說,我會選擇吃矮仔成來台南當然要吃個虱目魚才不枉此行虱目魚皮湯美味上桌,原以為只有魚皮的部分,沒想到魚皮上還有一層薄肉魚皮肉切片和少許薑絲清煮進嘴裡的層次感更驚豔,Q彈的膠質在皮本身表露無遺,而魚肉的部分卻有種咀嚼豬肉片的錯覺無刺,不過還是細嚼為妙,清爽不油,健康美味綜合鹹粥一端到眼前就看到它的實在感多種配料聚於一身,輕輕一撈,整湯匙的好料盡在眼底接著放入口中.....吃得出有煎的和煮的虱目魚、魚皮、魚肚、土魠魚、韭菜和蒜香,整個味蕾超滿足食材配料相當豐富,米粒本身感覺上反而只是配角這碗府城的道地早餐,總算見識了他的魅力,也讓咱家大飽口福~更多資訊請見【阿堂鹹粥】FB~記錄Chengchun好吃&好玩相片集,歡迎參觀~'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "with open(\"D:\\Data\\JsonData\\TainanFood\\TainanUnique.json\") as f:\n",
    "    allcontent=json.load(f)\n",
    "allcontent[2][\"comment\"][6][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\WordLibrary\\JiebaUse\\jieba_dict.txt.big ...\n",
      "2017-08-28 14:30:49,448 : DEBUG : Building prefix dict from D:\\WordLibrary\\JiebaUse\\jieba_dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Java\\AppData\\Local\\Temp\\jieba.uf3aabe1eaf7c86dda8313189b0c5c683.cache\n",
      "2017-08-28 14:30:49,453 : DEBUG : Loading model from cache C:\\Users\\Java\\AppData\\Local\\Temp\\jieba.uf3aabe1eaf7c86dda8313189b0c5c683.cache\n",
      "Loading model cost 1.107 seconds.\n",
      "2017-08-28 14:30:50,560 : DEBUG : Loading model cost 1.107 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2017-08-28 14:30:50,561 : DEBUG : Prefix dict has been built succesfully.\n",
      "2017-08-28 14:30:55,849 : INFO : 已完成前 1000 篇的斷詞\n",
      "2017-08-28 14:31:00,864 : INFO : 已完成前 2000 篇的斷詞\n",
      "2017-08-28 14:31:05,992 : INFO : 已完成前 3000 篇的斷詞\n",
      "2017-08-28 14:31:11,474 : INFO : 已完成前 4000 篇的斷詞\n",
      "2017-08-28 14:31:17,034 : INFO : 已完成前 5000 篇的斷詞\n",
      "2017-08-28 14:31:22,526 : INFO : 已完成前 6000 篇的斷詞\n",
      "2017-08-28 14:31:28,210 : INFO : 已完成前 7000 篇的斷詞\n",
      "2017-08-28 14:31:33,293 : INFO : 已完成前 8000 篇的斷詞\n",
      "2017-08-28 14:31:38,981 : INFO : 已完成前 9000 篇的斷詞\n",
      "2017-08-28 14:31:44,399 : INFO : 已完成前 10000 篇的斷詞\n",
      "2017-08-28 14:31:49,835 : INFO : 已完成前 11000 篇的斷詞\n",
      "2017-08-28 14:31:55,338 : INFO : 已完成前 12000 篇的斷詞\n",
      "2017-08-28 14:32:00,401 : INFO : 已完成前 13000 篇的斷詞\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-553e4020cf82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mcontent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"content\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwordset\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'\\u4e00'\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;34m'\\u9fff'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[1;34m(self, sentence, cut_all, HMM)\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    250\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mrecognized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m                             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecognized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m                                 \u001b[1;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m__cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\finalseg\\__init__.py\u001b[0m in \u001b[0;36m__cut\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnexti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m# print pos_list, sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'B'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# jieba custom setting.\n",
    "jieba.set_dictionary('D:/WordLibrary/JiebaUse/jieba_dict.txt.big')\n",
    "\n",
    "# load stopwords set\n",
    "stopwordset = set()\n",
    "with open('D:/WordLibrary/JiebaUse/stopwords.txt','r',encoding='utf-8') as sw:\n",
    "    for line in sw:\n",
    "        stopwordset.add(line.strip('\\n'))\n",
    "#---------------------------------------------------------------------------台南小吃名詞\n",
    "with open(\"D:\\Data\\JsonData\\TainanFood\\TainanUnique.json\",encoding='utf-8') as f:\n",
    "    contentss=json.load(f)\n",
    "Dienlist=[i[\"name\"] for i in contentss]\n",
    "stylelist=[i[\"style\"] for i in contentss]\n",
    "for i in Dienlist:\n",
    "    jieba.add_word(i)\n",
    "for i in stylelist:\n",
    "    jieba.add_word(i)\n",
    "\n",
    "#---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "output = open('D:\\Data\\word2VecData\\TainanContent.txt','w',encoding=\"utf-8\")\n",
    "\n",
    "texts_num = 0\n",
    "\n",
    "for dien in contentss:\n",
    "    for comment in dien['comment']:\n",
    "        content=comment[\"content\"]\n",
    "        words = jieba.cut(content, cut_all=False)\n",
    "        for word in words:\n",
    "            if word not in stopwordset and '\\u4e00' <= word and word <= '\\u9fff':\n",
    "                output.write(word +' ')\n",
    "        texts_num += 1\n",
    "        if texts_num % 10000 == 0:\n",
    "            logging.info(\"已完成前 %d 篇的斷詞\" % texts_num)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 只使用臺南做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(\"D:/Data/word2VecData/TainanContent.txt\")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=300,window=6, min_count=4, workers=4,sg=1)\n",
    "# sentences:當然了，這是要訓練的句子集，沒有他就不用跑了\n",
    "# size:這表示的是訓練出的詞向量會有幾維\n",
    "# alpha:機器學習中的學習率，這東西會逐漸收斂到 min_alpha\n",
    "# sg:這個不是三言兩語能說完的，sg=1表示採用skip-gram,sg=0 表示採用cbow\n",
    "# window:還記得孔乙己的例子嗎？能往左往右看幾個字的意思\n",
    "# workers:執行緒數目，除非電腦不錯，不然建議別超過 4\n",
    "# min_count:若這個詞出現的次數小於min_count，那他就不會被視為訓練對象\n",
    "\n",
    "# Save our model.\n",
    "model.save(\"D:/word2Vec/Tainan.model.bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = models.Word2Vec.load('D:/Data/word2VecData/Tainan.model.bin')\n",
    "\n",
    "print(\"提供 3 種測試模式\\n\")\n",
    "print(\"輸入一個詞，則去尋找前一百個該詞的相似詞\")\n",
    "print(\"輸入兩個詞，則去計算兩個詞的餘弦相似度\")\n",
    "print(\"輸入三個詞，進行類比推理\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input()\n",
    "        q_list = query.split()\n",
    "\n",
    "        if len(q_list) == 1:\n",
    "            print(\"相似詞前 100 排序\")\n",
    "            res = model.most_similar(q_list[0],topn = 100)\n",
    "            for item in res:\n",
    "                print(item[0]+\",\"+str(item[1]))\n",
    "\n",
    "        elif len(q_list) == 2:\n",
    "            print(\"計算 Cosine 相似度\")\n",
    "            res = model.similarity(q_list[0],q_list[1])\n",
    "            print(res)\n",
    "        else:\n",
    "            print(\"%s之於%s，如%s之於\" % (q_list[0],q_list[2],q_list[1]))\n",
    "            res = model.most_similar([q_list[0],q_list[1]], [q_list[2]], topn= 100)\n",
    "            for item in res:\n",
    "                print(item[0]+\",\"+str(item[1]))\n",
    "        print(\"----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from gensim.models import word2vec\n",
    "# from gensim import models\n",
    "# import logging\n",
    "# model = models.Word2Vec.load('D:/Data/word2VecData/Tainan.model.bin')\n",
    "# # model.similar_by_word(\"女兒\",topn=10)\n",
    "# model.similarity(\"老婆\",\"女兒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用臺南評論+wiki一起丟進去使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將維基百科詞庫加入台南詞庫>>一起訓練\n",
    "with open('D:/Data/word2VecData/TainanContent.txt',encoding=\"utf-8\") as f:\n",
    "    x=f.read()\n",
    "with open('D:/Data/word2VecData/wiki_seg.txt',encoding=\"utf-8\") as g:\n",
    "    y=g.read()\n",
    "    \n",
    "z=x+\" \"+y\n",
    "# type(z)\n",
    "with open('D:/Data/word2VecData/BigTainan.txt',\"w\",encoding=\"utf-8\") as h:\n",
    "    h.write(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(\"D:/Data/word2VecData/BigTainan.txt\")\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size=300,window=6, min_count=4, workers=4,sg=1)\n",
    "# sentences:當然了，這是要訓練的句子集，沒有他就不用跑了\n",
    "# size:這表示的是訓練出的詞向量會有幾維\n",
    "# alpha:機器學習中的學習率，這東西會逐漸收斂到 min_alpha\n",
    "# sg:這個不是三言兩語能說完的，sg=1表示採用skip-gram,sg=0 表示採用cbow\n",
    "# window:還記得孔乙己的例子嗎？能往左往右看幾個字的意思\n",
    "# workers:執行緒數目，除非電腦不錯，不然建議別超過 4\n",
    "# min_count:若這個詞出現的次數小於min_count，那他就不會被視為訓練對象\n",
    "\n",
    "# Save our model.\n",
    "model.save(\"D:/Data/word2VecData/BigTainan.model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = models.Word2Vec.load('D:/Data/word2VecData/BigTainan2.model.bin')\n",
    "\n",
    "print(\"提供 3 種測試模式\\n\")\n",
    "print(\"輸入一個詞，則去尋找前一百個該詞的相似詞\")\n",
    "print(\"輸入兩個詞，則去計算兩個詞的餘弦相似度\")\n",
    "print(\"輸入三個詞，進行類比推理\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input()\n",
    "        q_list = query.split()\n",
    "\n",
    "        if len(q_list) == 1:\n",
    "            print(\"相似詞前 30 排序\")\n",
    "            res = model.most_similar(q_list[0],topn = 30)\n",
    "            for item in res:\n",
    "                print(item[0]+\",\"+str(item[1]))\n",
    "\n",
    "        elif len(q_list) == 2:\n",
    "            print(\"計算 Cosine 相似度\")\n",
    "            res = model.similarity(q_list[0],q_list[1])\n",
    "            print(res)\n",
    "        else:\n",
    "            print(\"%s之於%s，如%s之於\" % (q_list[0],q_list[2],q_list[1]))\n",
    "            res = model.most_similar([q_list[0],q_list[1]], [q_list[2]], topn= 30)\n",
    "            for item in res:\n",
    "                print(item[0]+\",\"+str(item[1]))\n",
    "        print(\"----------------------------\")\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-29 20:32:58,314 : INFO : loading Word2Vec object from D:/Data/word2VecData/BigTainan2.model.bin\n",
      "2017-08-29 20:33:00,226 : INFO : loading wv recursively from D:/Data/word2VecData/BigTainan2.model.bin.wv.* with mmap=None\n",
      "2017-08-29 20:33:00,227 : INFO : loading syn0 from D:/Data/word2VecData/BigTainan2.model.bin.wv.syn0.npy with mmap=None\n",
      "2017-08-29 20:33:00,714 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-29 20:33:00,714 : INFO : loading syn1neg from D:/Data/word2VecData/BigTainan2.model.bin.syn1neg.npy with mmap=None\n",
      "2017-08-29 20:33:01,290 : INFO : setting ignored attribute cum_table to None\n",
      "2017-08-29 20:33:01,291 : INFO : loaded D:/Data/word2VecData/BigTainan2.model.bin\n",
      "2017-08-29 20:33:03,731 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('王位', 0.6161760687828064),\n",
       " ('王國', 0.5841505527496338),\n",
       " ('君主', 0.5829795002937317),\n",
       " ('王室', 0.5826740860939026),\n",
       " ('王儲', 0.5746091604232788),\n",
       " ('二世', 0.5744969844818115),\n",
       " ('一世', 0.5662062168121338),\n",
       " ('王共治', 0.5638306140899658),\n",
       " ('王太子', 0.5568482279777527),\n",
       " ('艾瑪王', 0.5512856245040894),\n",
       " ('加休曼', 0.5455122590065002),\n",
       " ('王後和', 0.5407955050468445),\n",
       " ('明焰', 0.5406404733657837),\n",
       " ('共治者', 0.5357548594474792),\n",
       " ('加冕', 0.5349020957946777),\n",
       " ('謝普隆', 0.5344705581665039),\n",
       " ('敏東', 0.5341904759407043),\n",
       " ('吉增', 0.5336169004440308),\n",
       " ('伴妃', 0.533092200756073),\n",
       " ('新王', 0.5326541066169739),\n",
       " ('王後則', 0.5265640616416931),\n",
       " ('懶王', 0.5258197784423828),\n",
       " ('妖連', 0.5258163213729858),\n",
       " ('萊齊耶', 0.5246374607086182),\n",
       " ('美哈梅', 0.524246871471405),\n",
       " ('薩旺', 0.5228676199913025),\n",
       " ('加冕典禮', 0.5220558643341064),\n",
       " ('亞蓋隆', 0.5217757225036621),\n",
       " ('岡比西斯', 0.5199179649353027),\n",
       " ('卡紐特', 0.5199088454246521)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = models.Word2Vec.load('D:/Data/word2VecData/BigTainan2.model.bin')\n",
    "model.most_similar(positive=['女生','國王'],negative=[\"男生\"],topn = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('阿松割包', 0.8017070889472961),\n",
       " ('進福炒鱔魚專家', 0.796049952507019),\n",
       " ('武廟肉圓', 0.7932929992675781),\n",
       " ('賣小卷', 0.7828423380851746),\n",
       " ('永樂燒肉飯', 0.7819386720657349),\n",
       " ('這小卷', 0.7819290161132812),\n",
       " ('割包', 0.7812110185623169),\n",
       " ('阿明豬心', 0.7791026830673218),\n",
       " ('醇涎坊', 0.7787204384803772),\n",
       " ('蝦卷', 0.7784358859062195),\n",
       " ('邱家', 0.77752685546875),\n",
       " ('阿松', 0.7771453261375427),\n",
       " ('合豐', 0.7766473889350891),\n",
       " ('北斗', 0.7758220434188843),\n",
       " ('集品蝦仁飯', 0.7734678387641907),\n",
       " ('石精臼蚵仔煎', 0.7729687094688416),\n",
       " ('楊家', 0.771555483341217),\n",
       " ('旗哥', 0.7711482644081116),\n",
       " ('剛吃過', 0.7707981467247009),\n",
       " ('員林肉圓', 0.7697001099586487),\n",
       " ('夏家魚麵', 0.7691518664360046),\n",
       " ('保安路米糕', 0.7691022753715515),\n",
       " ('這攤', 0.7688997387886047),\n",
       " ('金得春捲', 0.7684884071350098),\n",
       " ('人吃過', 0.7680496573448181),\n",
       " ('担仔', 0.7679417133331299),\n",
       " ('克林', 0.7677850723266602),\n",
       " ('度小月擔仔麵', 0.7667901515960693),\n",
       " ('福記水煎包', 0.7667505741119385),\n",
       " ('烤魯味', 0.766660749912262)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['難吃','台南小吃','福記肉圓'],topn = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('阿松割包', 0.7485965490341187),\n",
       " ('阿明豬心', 0.7303024530410767),\n",
       " ('阿堂鹹粥', 0.7163146734237671),\n",
       " ('金得春捲', 0.705902099609375),\n",
       " ('石精臼蚵仔煎', 0.700342059135437),\n",
       " ('福記肉圓', 0.7000635266304016),\n",
       " ('文章牛肉湯', 0.6971155405044556),\n",
       " ('葉家小卷米粉', 0.689702033996582),\n",
       " ('度小月擔仔麵', 0.6864302754402161),\n",
       " ('王氏魚皮', 0.6839126944541931),\n",
       " ('富盛號碗粿', 0.6825461387634277),\n",
       " ('包成羊肉', 0.6770150065422058),\n",
       " ('阿憨鹹粥', 0.6744212508201599),\n",
       " ('武廟肉圓', 0.6739205718040466),\n",
       " ('茂雄蝦仁肉圓', 0.6683276295661926),\n",
       " ('永樂燒肉飯', 0.6674696803092957),\n",
       " ('赤崁棺材板', 0.6669817566871643),\n",
       " ('阿桐意麵', 0.6636193990707397),\n",
       " ('割包', 0.6610705852508545),\n",
       " ('江水號', 0.656991183757782),\n",
       " ('葉家', 0.656593382358551),\n",
       " ('集品蝦仁飯', 0.6540898084640503),\n",
       " ('醇涎坊鍋燒意麵', 0.6491363048553467),\n",
       " ('富勝號', 0.6491291522979736),\n",
       " ('土魠魚羹', 0.6479046940803528),\n",
       " ('保安路米糕', 0.6468154788017273),\n",
       " ('一味品碗粿', 0.6451788544654846),\n",
       " ('阿村牛肉湯', 0.6438093185424805),\n",
       " ('湯俗擱', 0.6436648368835449),\n",
       " ('擱賀架', 0.6411492824554443)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['台南小吃'],topn = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "with open(\"D:\\Data\\JsonData\\TainanFood\\TainanUnique.json\",encoding='utf-8') as f:\n",
    "    contentss=json.load(f)\n",
    "    dien_words=[i[\"name\"] for i in contentss]\n",
    "    style_words=[i[\"style\"] for i in contentss]\n",
    "\n",
    "def getWordVecs(words):\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            vecs.append(model[word].reshape((1,300)))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype='float') #TSNE expects float type values\n",
    "\n",
    "dien_vecs = getWordVecs(dien_words)\n",
    "style_vecs = getWordVecs(style_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ts = TSNE(2)\n",
    "reduced_vecs = ts.fit_transform(np.concatenate((dien_vecs,style_vecs)))\n",
    "\n",
    "#color points by word group to see if Word2Vec can separate them\n",
    "for i in range(len(reduced_vecs)):\n",
    "    if i < len(dien_vecs):\n",
    "        #food words colored blue\n",
    "        color = 'b'\n",
    "    elif i >= len(dien_vecs) and i < (len(dien_vecs) + len(style_vecs)):\n",
    "        #sports words colored red\n",
    "        color = 'r'\n",
    "    plt.plot(reduced_vecs[i,0], reduced_vecs[i,1], marker='o', color=color, markersize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
